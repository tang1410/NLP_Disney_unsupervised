{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f89801e9",
   "metadata": {},
   "source": [
    "# Open file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a848dce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Year_Month</th>\n",
       "      <th>Reviewer_Location</th>\n",
       "      <th>Review_Text</th>\n",
       "      <th>Branch</th>\n",
       "      <th>Continent</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-1</td>\n",
       "      <td>Australia</td>\n",
       "      <td>To all who comes to this happy place, welcome....</td>\n",
       "      <td>Disneyland_California</td>\n",
       "      <td>Oceania</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-2</td>\n",
       "      <td>United States</td>\n",
       "      <td>Many rides were closed for maintenance. Then o...</td>\n",
       "      <td>Disneyland_California</td>\n",
       "      <td>Amercia</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-2</td>\n",
       "      <td>United States</td>\n",
       "      <td>Broken down rides, unhappy employees, dirty. D...</td>\n",
       "      <td>Disneyland_California</td>\n",
       "      <td>Amercia</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-2</td>\n",
       "      <td>United States</td>\n",
       "      <td>We recently went to Balboa Island on vacation ...</td>\n",
       "      <td>Disneyland_California</td>\n",
       "      <td>Amercia</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-2</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Oh my gosh!! Where do I even begin. We drove t...</td>\n",
       "      <td>Disneyland_California</td>\n",
       "      <td>Amercia</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-12</td>\n",
       "      <td>United States</td>\n",
       "      <td>We love, love, love Disney World (Orlando, FL)...</td>\n",
       "      <td>Disneyland_California</td>\n",
       "      <td>Amercia</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-12</td>\n",
       "      <td>United States</td>\n",
       "      <td>We have read that coming to Disney on Christma...</td>\n",
       "      <td>Disneyland_California</td>\n",
       "      <td>Amercia</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-12</td>\n",
       "      <td>United States</td>\n",
       "      <td>Since the children were out of school and we w...</td>\n",
       "      <td>Disneyland_California</td>\n",
       "      <td>Amercia</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-2</td>\n",
       "      <td>United States</td>\n",
       "      <td>Disneyland has proven to be all about getting ...</td>\n",
       "      <td>Disneyland_California</td>\n",
       "      <td>Amercia</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-12</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Myself, along with my two chidren ages 8 and 1...</td>\n",
       "      <td>Disneyland_California</td>\n",
       "      <td>Amercia</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Rating Year_Month Reviewer_Location  \\\n",
       "0         1     2019-1         Australia   \n",
       "1         2     2019-2     United States   \n",
       "2         2     2019-2     United States   \n",
       "3         2     2019-2     United States   \n",
       "4         1     2019-2            Canada   \n",
       "..      ...        ...               ...   \n",
       "283       1    2011-12     United States   \n",
       "284       1    2011-12     United States   \n",
       "285       1    2010-12     United States   \n",
       "286       1     2011-2     United States   \n",
       "287       1    2010-12            Canada   \n",
       "\n",
       "                                           Review_Text                 Branch  \\\n",
       "0    To all who comes to this happy place, welcome....  Disneyland_California   \n",
       "1    Many rides were closed for maintenance. Then o...  Disneyland_California   \n",
       "2    Broken down rides, unhappy employees, dirty. D...  Disneyland_California   \n",
       "3    We recently went to Balboa Island on vacation ...  Disneyland_California   \n",
       "4    Oh my gosh!! Where do I even begin. We drove t...  Disneyland_California   \n",
       "..                                                 ...                    ...   \n",
       "283  We love, love, love Disney World (Orlando, FL)...  Disneyland_California   \n",
       "284  We have read that coming to Disney on Christma...  Disneyland_California   \n",
       "285  Since the children were out of school and we w...  Disneyland_California   \n",
       "286  Disneyland has proven to be all about getting ...  Disneyland_California   \n",
       "287  Myself, along with my two chidren ages 8 and 1...  Disneyland_California   \n",
       "\n",
       "    Continent  Season  \n",
       "0     Oceania  Winter  \n",
       "1     Amercia  Winter  \n",
       "2     Amercia  Winter  \n",
       "3     Amercia  Winter  \n",
       "4     Amercia  Winter  \n",
       "..        ...     ...  \n",
       "283   Amercia  Winter  \n",
       "284   Amercia  Winter  \n",
       "285   Amercia  Winter  \n",
       "286   Amercia  Winter  \n",
       "287   Amercia  Winter  \n",
       "\n",
       "[288 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "\n",
    "document = pandas.read_csv(\"C:/Users/user/OneDrive/Desktop/MK495/final project/Data/Winter_bad.csv\")\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace5645b",
   "metadata": {},
   "source": [
    "# Transform .csv to .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7623ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      To all who comes to this happy place, welcome....\n",
      "1      Many rides were closed for maintenance. Then o...\n",
      "2      Broken down rides, unhappy employees, dirty. D...\n",
      "3      We recently went to Balboa Island on vacation ...\n",
      "4      Oh my gosh!! Where do I even begin. We drove t...\n",
      "                             ...                        \n",
      "283    We love, love, love Disney World (Orlando, FL)...\n",
      "284    We have read that coming to Disney on Christma...\n",
      "285    Since the children were out of school and we w...\n",
      "286    Disneyland has proven to be all about getting ...\n",
      "287    Myself, along with my two chidren ages 8 and 1...\n",
      "Name: Review_Text, Length: 288, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "document.to_pickle(\"C:/Users/user/OneDrive/Desktop/MK495/final project/Data/Winter_bad.pkl\")\n",
    "documents = pickle.load(open(\"C:/Users/user/OneDrive/Desktop/MK495/final project/Data/Winter_bad.pkl\",\"rb\"))\n",
    "reviews = documents.Review_Text\n",
    "print(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a351d30f",
   "metadata": {},
   "source": [
    "# No. of Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3a35be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ReviewSample = reviews\n",
    "ReviewSample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b61372",
   "metadata": {},
   "source": [
    "# CleanedReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d031be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "CleanedReviews = ReviewSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef99a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8fd7ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-fdbac64ec2d6>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  CleanedReviews[i] = (\" \").join(stems)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      come happi place , welcom . walt vision lost w...\n",
       "1      ride close mainten . then ride shut wait line ...\n",
       "2      broken ride , unhappi employe , dirti . defini...\n",
       "3      recent went balboa island vacat famili . also ...\n",
       "4      oh gosh ! ! where even begin . drove la northe...\n",
       "                             ...                        \n",
       "283    love , love , love disney world ( orlando , fl...\n",
       "284    read come disney christma would great experi f...\n",
       "285    children school work holiday decid celebr gran...\n",
       "286    disneyland proven all get money not care good ...\n",
       "287    , chidren age 8 17 visit disneyland california...\n",
       "Name: Review_Text, Length: 288, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range (288):\n",
    "    CleanedTokens = [token.text for token in nlp(ReviewSample[i]) if not token.is_stop or token.pos_ in {'ADV','VERB'}]\n",
    "    stems = [stemmer.stem(token) for token in CleanedTokens]\n",
    "    CleanedReviews[i] = (\" \").join(stems)\n",
    "CleanedReviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28abb407",
   "metadata": {},
   "source": [
    "# Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73e31e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = 'word')\n",
    "\n",
    "vectorizer.fit(CleanedReviews)\n",
    "vector = vectorizer.transform(CleanedReviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f67c763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 3223)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f8ddf1",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a33783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(3,5),min_df=0.001,max_df=0.80,stop_words='english',sublinear_tf=True)\n",
    "vector = tfidf_vectorizer.fit_transform(CleanedReviews)\n",
    "idf = tfidf_vectorizer.idf_\n",
    "tfidf = vector.sum(axis=0).A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9b2ece0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.97327951 5.97327951 5.97327951 ... 5.97327951 5.97327951 5.97327951]\n"
     ]
    }
   ],
   "source": [
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8b456cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07204405 0.07204405 0.07204405 ... 0.07502736 0.07502736 0.07502736]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb6b4c",
   "metadata": {},
   "source": [
    "# Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a62fed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07204405 0.07204405 0.07204405 ... 0.07502736 0.07502736 0.07502736]]\n"
     ]
    }
   ],
   "source": [
    "SumFeature = vector.sum(axis=0)\n",
    "print(SumFeature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d566e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dictionary = tfidf_vectorizer.vocabulary_\n",
    "findword = {value:key for key,value in Dictionary.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8dde65",
   "metadata": {},
   "source": [
    "# Silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5561346f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00011034530031822748, 0.00012171585776167696, 0.00012318911295749522, 0.0004508674710594997, 0.0006314839223338906, 0.00011996264677768501, 0.0013082294137311673, 0.0008744037366824422, 0.001310464506286622, 0.001456468220829362]\n"
     ]
    }
   ],
   "source": [
    "#Optimal Number of cluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "scores = []\n",
    "for n_clusters in range(10):\n",
    "    kmsclust = KMeans(n_clusters=n_clusters+2, random_state = 123)\n",
    "    clusters = kmsclust.fit_predict(vector.toarray())\n",
    "    \n",
    "    scores.append(silhouette_score(vector.toarray(),clusters))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5659b717",
   "metadata": {},
   "source": [
    "# Find max scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d4bdd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st = 0.001456468220829362 at [9]\n"
     ]
    }
   ],
   "source": [
    "theset = frozenset(scores)\n",
    "theset = sorted(theset, reverse=True)\n",
    "thedict = {}\n",
    "for j in range(10):\n",
    "    positions = [i for i, x in enumerate(scores) if x == theset[j]]\n",
    "    thedict[theset[j]] = positions\n",
    "\n",
    "print('1st = ' + str(theset[0]) + ' at ' + str(thedict.get(theset[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807a2689",
   "metadata": {},
   "source": [
    "# KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea59457a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMclusters: [0 0 0 0 0 0 0 3 0 0 0 1 0 0 1 0 0 0 6 0 0 0 4 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 4 0 0 1 0 0 0 0 1 0 3 0 4 0 0 4 0 1 0 0 0 4 0 2 0 0 0\n",
      " 4 0 0 0 0 0 0 0 0 0 2 0 0 4 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 3 0 0 1 1 5 3 2 0 6 0 0 0 0 0 0 0 0 0 0 6 0\n",
      " 0 0 0 0 0 0 0 0 4 0 0 1 0 1 1 0 0 0 0 4 3 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n",
      " 1 0 6 0 0 0 0 0 6 0 0 0 0 0 0 1 0 0 5 3 0 0 0 4 5 0 0 0 4 4 0 6 4 0 0 4 0\n",
      " 4 6 0 6 1 5 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 6 4 0 0 0 0 0 3 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 3 0 0 1 0 0 6 1 0 0 2 0 0 4 0 0 0 0 0 0 0]\n",
      " \n",
      "KMcentroids: [[ 3.32000248e-04  3.32000248e-04  3.32000248e-04 ...  3.45748214e-04\n",
      "   3.45748214e-04  3.45748214e-04]\n",
      " [-5.42101086e-20 -5.42101086e-20 -5.42101086e-20 ... -5.42101086e-20\n",
      "  -5.42101086e-20 -5.42101086e-20]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [ 5.42101086e-20  5.42101086e-20  5.42101086e-20 ...  5.42101086e-20\n",
      "   5.42101086e-20  5.42101086e-20]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "kms = KMeans(n_clusters = 7, random_state = 123)\n",
    "clusters = kms.fit_predict(vector.toarray())\n",
    "centroids = kms.cluster_centers_\n",
    "print(\"KMclusters:\", clusters)\n",
    "print(\" \")\n",
    "print(\"KMcentroids:\", centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e9416f",
   "metadata": {},
   "source": [
    "# Centroid population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d004b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 217, 1: 28, 4: 17, 6: 10, 3: 8, 2: 4, 5: 4})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "count = Counter(clusters)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb361da5",
   "metadata": {},
   "source": [
    "# Assign Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f69b9a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 64419)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c301dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids1 = centroids[0]\n",
    "centroids2 = centroids[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e068d28f",
   "metadata": {},
   "source": [
    "# Find most word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "893bc8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st = 0.0019301635407395594 at [61662]\n",
      "2nd = 0.0016549785547554808 at [37318]\n",
      "3rd = 0.001544323939726291 at [15256]\n",
      "4th = 0.001348387689173169 at [20035]\n",
      "5th = 0.0013137024284305629 at [47147]\n"
     ]
    }
   ],
   "source": [
    "theset = frozenset(centroids1)\n",
    "theset = sorted(theset, reverse=True)\n",
    "thedict = {}\n",
    "for j in range(5):\n",
    "    positions = [i for i, x in enumerate(centroids1) if x == theset[j]]\n",
    "    thedict[theset[j]] = positions\n",
    "\n",
    "print('1st = ' + str(theset[0]) + ' at ' + str(thedict.get(theset[0])))\n",
    "print('2nd = ' + str(theset[1]) + ' at ' + str(thedict.get(theset[1])))\n",
    "print('3rd = ' + str(theset[2]) + ' at ' + str(thedict.get(theset[2])))\n",
    "print('4th = ' + str(theset[3]) + ' at ' + str(thedict.get(theset[3])))\n",
    "print('5th = ' + str(theset[4]) + ' at ' + str(thedict.get(theset[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3da74925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wast time money\n",
      "onli got ride\n",
      "disneyland california adventur\n",
      "fast pass ride\n",
      "ride fast pass\n"
     ]
    }
   ],
   "source": [
    "for p in range(5):\n",
    "    a = (thedict.get(theset[p]))\n",
    "    b = a[0]\n",
    "    print(findword[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3fc2b76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st = 0.0408643247145017 at [24050]\n",
      "2nd = 0.007154844215325071 at [63658]\n",
      "3rd = 0.006893301072242143 at [49844]\n",
      "4th = 0.006322192387435294 at [11527, 11528, 11529, 16229, 16781, 16782, 16783, 16808, 16809, 16810, 24063, 24064, 25505, 25506, 25507, 27446, 27447, 30866, 30867, 30868, 42638, 42639, 42657, 42658, 42659, 60263, 60264, 63659, 63660]\n",
      "5th = 0.005893043137578341 at [42635]\n"
     ]
    }
   ],
   "source": [
    "theset = frozenset(centroids2)\n",
    "theset = sorted(theset, reverse=True)\n",
    "thedict = {}\n",
    "for j in range(5):\n",
    "    positions = [i for i, x in enumerate(centroids2) if x == theset[j]]\n",
    "    thedict[theset[j]] = positions\n",
    "\n",
    "print('1st = ' + str(theset[0]) + ' at ' + str(thedict.get(theset[0])))\n",
    "print('2nd = ' + str(theset[1]) + ' at ' + str(thedict.get(theset[1])))\n",
    "print('3rd = ' + str(theset[2]) + ' at ' + str(thedict.get(theset[2])))\n",
    "print('4th = ' + str(theset[3]) + ' at ' + str(thedict.get(theset[3])))\n",
    "print('5th = ' + str(theset[4]) + ' at ' + str(thedict.get(theset[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5bf96d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happiest place earth\n",
      "worth wait hour\n",
      "secur check line\n",
      "crowd place earth\n",
      "place earth crowd\n"
     ]
    }
   ],
   "source": [
    "for p in range(5):\n",
    "    a = (thedict.get(theset[p]))\n",
    "    b = a[0]\n",
    "    print(findword[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03484e6",
   "metadata": {},
   "source": [
    "# Sihoutte_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58136717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0038650581442779644, 0.0038592140184737483, 0.0038946883969717453, 0.003944061320296975, 0.004003886954816108, 0.004032079846386222, 0.00408015395988422]\n"
     ]
    }
   ],
   "source": [
    "#Optimal Number of cluster\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "scores = []\n",
    "for n_clusters in range(7):\n",
    "    aggclust = AgglomerativeClustering(n_clusters=n_clusters+2)\n",
    "    clusters = aggclust.fit_predict(vector.toarray())\n",
    "    \n",
    "    scores.append(silhouette_score(vector.toarray(),clusters))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963b9b3a",
   "metadata": {},
   "source": [
    "# Find max score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b992b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st = 0.00408015395988422 at [6]\n"
     ]
    }
   ],
   "source": [
    "theset = frozenset(scores)\n",
    "theset = sorted(theset, reverse=True)\n",
    "thedict = {}\n",
    "for j in range(5):\n",
    "    positions = [i for i, x in enumerate(scores) if x == theset[j]]\n",
    "    thedict[theset[j]] = positions\n",
    "\n",
    "print('1st = ' + str(theset[0]) + ' at ' + str(thedict.get(theset[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955d761",
   "metadata": {},
   "source": [
    "# Agglo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dfb52b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy\n",
    "aggclust = AgglomerativeClustering(n_clusters = 7)\n",
    "clusters = aggclust.fit_predict(vector.toarray())\n",
    "centroids = numpy.array([vector[clusters == c].mean(axis=0).A1 for c in range(6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e7468b4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agglo_clusters: [0 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0 6\n",
      " 0 0 0 0 0 0 0 3 0 0 1 0 2 6 0 0 6 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 3 0 5 0 0 0 4 0 0 0 0 0 0 0 0 0 1 0 6 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 6 0 2 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 2 2 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 2\n",
      " 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 6 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0\n",
      " 0 0 0 0 6 0 0 2 2 0 0 0 0 0 0 0 0 1 0 6 1 0 2 0 0 0 0 0 0 0 0 0 0 6 0 0 0\n",
      " 0 0 0 0 0 0 0 0 2 0 0 0 5 0 0 6 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " \n",
      "Agglo_centroids: [[0.         0.         0.         ... 0.00030499 0.00030499 0.00030499]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Agglo_clusters:\", \n",
    "      clusters)\n",
    "print(\" \")\n",
    "print(\"Agglo_centroids:\", \n",
    "      centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0126a02",
   "metadata": {},
   "source": [
    "# Centroid population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f85327e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 246, 6: 13, 1: 12, 2: 11, 3: 2, 5: 2, 4: 2})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "count = Counter(clusters)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c489489",
   "metadata": {},
   "source": [
    "# Assign Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd654f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids1 = centroids[0]\n",
    "centroids2 = centroids[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddddcb43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st = 0.09808594318546617 at [52002, 52003]\n",
      "2nd = 0.08908693396644626 at [13032]\n",
      "3rd = 0.05480433218175117 at [3334, 3335, 3336, 10027, 10028, 10029, 11521, 11522, 11523, 13035, 13036, 13239, 13240, 13241, 14502, 14503, 14504, 14644, 14645, 14646, 15113, 15114, 15115, 15424, 15425, 15426, 17457, 17458, 17459, 18712, 18713, 18714, 20573, 20574, 20575, 27745, 29410, 29411, 29412, 29493, 29494, 29495, 29999, 30000, 30001, 30128, 30129, 30130, 31370, 31371, 36800, 36801, 36802, 39544, 39545, 41806, 41807, 41808, 42790, 42791, 42792, 45571, 45572, 45573, 47873, 47874, 47875, 52004, 54588, 54589, 54590, 57974, 57975, 57976, 60374, 60375, 60376, 63402, 63403, 63404]\n",
      "4th = 0.050424526464682144 at [1752, 1753, 1754, 2209, 2210, 2211, 9835, 9836, 9837, 9936, 9937, 9938, 13043, 13044, 14971, 14972, 14973, 22054, 22055, 22056, 23946, 23947, 23948, 24281, 24282, 24283, 27596, 27597, 27598, 28806, 28807, 28808, 28917, 28918, 28919, 30250, 30251, 30252, 33033, 33034, 33035, 37871, 37872, 37873, 39371, 39372, 40050, 40051, 46612, 46613, 46614, 46959, 46960, 46961, 47602, 47603, 47604, 49212, 49267, 49268, 49269, 50566, 50567, 50568, 52005, 53524, 53525, 53526, 55998, 55999, 56000, 56210, 56211, 56212, 56478, 56479, 56480, 56704, 56705, 56706, 57287, 57288, 57289, 58146, 58147, 58148, 58353, 58354, 58355, 60305, 60306, 60307, 60404, 60405, 60406]\n",
      "5th = 0.04844476566787533 at [31367]\n"
     ]
    }
   ],
   "source": [
    "theset = frozenset(centroids2)\n",
    "theset = sorted(theset, reverse=True)\n",
    "thedict = {}\n",
    "for j in range(5):\n",
    "    positions = [i for i, x in enumerate(centroids2) if x == theset[j]]\n",
    "    thedict[theset[j]] = positions\n",
    "\n",
    "print('1st = ' + str(theset[0]) + ' at ' + str(thedict.get(theset[0])))\n",
    "print('2nd = ' + str(theset[1]) + ' at ' + str(thedict.get(theset[1])))\n",
    "print('3rd = ' + str(theset[2]) + ' at ' + str(thedict.get(theset[2])))\n",
    "print('4th = ' + str(theset[3]) + ' at ' + str(thedict.get(theset[3])))\n",
    "print('5th = ' + str(theset[4]) + ' at ' + str(thedict.get(theset[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07e49389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spent day wait\n",
      "day wait line\n",
      "amus park just\n",
      "80 head action\n",
      "long line crowd\n"
     ]
    }
   ],
   "source": [
    "for p in range(5):\n",
    "    a = (thedict.get(theset[p]))\n",
    "    b = a[0]\n",
    "    print(findword[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e97661c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st = 0.0019320159431115878 at [24050]\n",
      "2nd = 0.0017618215766459723 at [60763]\n",
      "3rd = 0.001702623936343432 at [61662]\n",
      "4th = 0.001505065820276217 at [47147]\n",
      "5th = 0.0015042953926954212 at [15093]\n"
     ]
    }
   ],
   "source": [
    "theset = frozenset(centroids1)\n",
    "theset = sorted(theset, reverse=True)\n",
    "thedict = {}\n",
    "for j in range(5):\n",
    "    positions = [i for i, x in enumerate(centroids1) if x == theset[j]]\n",
    "    thedict[theset[j]] = positions\n",
    "\n",
    "print('1st = ' + str(theset[0]) + ' at ' + str(thedict.get(theset[0])))\n",
    "print('2nd = ' + str(theset[1]) + ' at ' + str(thedict.get(theset[1])))\n",
    "print('3rd = ' + str(theset[2]) + ' at ' + str(thedict.get(theset[2])))\n",
    "print('4th = ' + str(theset[3]) + ' at ' + str(thedict.get(theset[3])))\n",
    "print('5th = ' + str(theset[4]) + ' at ' + str(thedict.get(theset[4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe30b0b",
   "metadata": {},
   "source": [
    "# Find most words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ea4ea963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happiest place earth\n",
      "wait time ride\n",
      "wast time money\n",
      "ride fast pass\n",
      "disney world orlando\n"
     ]
    }
   ],
   "source": [
    "for p in range(5):\n",
    "    a = (thedict.get(theset[p]))\n",
    "    b = a[0]\n",
    "    print(findword[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d680b14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
